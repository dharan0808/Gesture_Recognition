{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import GestureRecognitionModel"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_frames=20, transform=None, gesture_to_label=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.gesture_to_label = gesture_to_label or {\n",
    "            gesture.name: idx for idx, gesture in enumerate(sorted(self.root_dir.iterdir()))\n",
    "        }\n",
    "\n",
    "        for gesture in self.root_dir.iterdir():\n",
    "            label = self.gesture_to_label[gesture.name]\n",
    "            frames = sorted(list(gesture.glob(\"*.jpg\")))\n",
    "            if len(frames) >= num_frames:\n",
    "                self.samples.append((frames[:num_frames], label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "        frames = [self.transform(Image.open(p).convert(\"RGB\")) for p in frame_paths]\n",
    "        clip = torch.stack(frames)  # shape: [num_frames, 3, 224, 224]\n",
    "        return clip, label"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "gesture_to_label = {'Fist': 0, 'Four': 1, 'Me': 2, 'One': 3, 'Small': 4}\n",
    "train_dir = \"data/frames/train\"\n",
    "val_dir = \"data/frames/val\"\n",
    "\n",
    "train_dataset = GestureDataset(train_dir, num_frames=16, transform=train_transform, gesture_to_label=gesture_to_label)\n",
    "val_dataset   = GestureDataset(val_dir, num_frames=4,  transform=val_transform, gesture_to_label=gesture_to_label)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_classes = len(gesture_to_label)\n",
    "model = GestureRecognitionModel(num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=2)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "no_improve_epochs = 0\n",
    "patience = 5\n",
    "num_epochs = 30\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        model.train() if phase == 'train' else model.eval()\n",
    "        dataloader = train_loader if phase == 'train' else val_loader\n",
    "\n",
    "        running_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "        if phase == \"val\":\n",
    "            scheduler.step(epoch_loss)\n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), \"gesture_model.pth\")\n",
    "                print(\"Validation accuracy improved. Model saved.\")\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "\n",
    "    if no_improve_epochs >= patience:\n",
    "        print(\"\\nEarly stopping triggered.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_acc:.4f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


